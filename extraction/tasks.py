# # extraction/tasks.py - Version nettoy√©e et optimis√©e


# extraction/tasks.py - Ajout de la fonction process_document_from_urls
import logging
from datetime import datetime
from typing import Dict, List, Optional

from django.core.files.storage import default_storage
from django.utils import timezone

from documents.models import Document
from .models import ExtractionResult
from .services import MistralAIService, DocumentTextExtractor, NLPAnnotationService
from .url_services import URLDocumentProcessor

logger = logging.getLogger(__name__)

# Import conditionnel de Celery
try:
    from celery import shared_task

    CELERY_AVAILABLE = True
except ImportError:
    CELERY_AVAILABLE = False


    def shared_task(func):
        return func


# [Toutes les autres classes et fonctions existantes restent inchang√©es...]
# DocumentProcessor, DocumentUpdater, ConfidenceCalculator, etc.

class DocumentProcessor:
    """Classe pour traiter les documents de mani√®re coh√©rente"""

    def __init__(self):
        self.text_extractor = DocumentTextExtractor()
        self.metadata_service = MistralAIService()
        self.nlp_service = NLPAnnotationService()

    def validate_document(self, document: Document) -> Dict[str, any]:
        validation_result = {
            'valid': False,
            'error': None,
            'file_path': None,
            'file_size': 0,
            'has_urls': False
        }

        if document.file:
            try:
                file_path = document.file.path
                import os

                if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                    validation_result.update({
                        'valid': True,
                        'file_path': file_path,
                        'file_size': os.path.getsize(file_path),
                        'has_urls': False
                    })
                    logger.info(f"üìÇ Fichier local valide d√©tect√©: {file_path}")
                    return validation_result
                else:
                    validation_result['error'] = "Fichier local introuvable ou vide"
                    return validation_result

            except Exception as e:
                validation_result['error'] = f"Erreur acc√®s fichier local: {str(e)}"
                return validation_result

        elif document.direct_pdf_url and document.ema_page_url:
            validation_result.update({
                'valid': True,
                'has_urls': True
            })
            logger.info(f"üåê URLs valides d√©tect√©es pour le document {document.id}")
            return validation_result

        else:
            validation_result['error'] = "Aucun fichier local ni URLs valides"
            return validation_result


class DocumentUpdater:
    """Classe pour mettre √† jour les documents avec les m√©tadonn√©es extraites"""

    @staticmethod
    def safe_string_field(value: any, max_length: int) -> str:
        """S√©curise un champ string pour la base de donn√©es"""
        if not value:
            return ""
        value_str = str(value).strip()
        return value_str[:max_length - 3] + "..." if len(value_str) > max_length else value_str

    @staticmethod
    def parse_date(date_str: str) -> Optional[datetime]:
        """Parse une date de mani√®re robuste"""
        if not date_str or not str(date_str).strip():
            return None

        date_str = str(date_str).strip()

        # Ignorer les valeurs sp√©ciales
        special_values = ['YEAR_MISSING', 'DATE_MISSING', 'N/A', 'None', '', 'null']
        if date_str.lower() in [v.lower() for v in special_values]:
            return None

        try:
            # Essayer avec dateparser si disponible
            try:
                import dateparser
                parsed_date = dateparser.parse(date_str, languages=['fr', 'en'])
                if parsed_date:
                    return parsed_date
            except ImportError:
                pass

            # Formats manuels
            import re

            # ISO format
            if re.match(r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}', date_str):
                return datetime.fromisoformat(date_str.rstrip('Z'))

            # Format simple ISO
            if re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
                return datetime.strptime(date_str, '%Y-%m-%d')

            # Format DD/MM/YYYY
            if re.match(r'^\d{1,2}/\d{1,2}/\d{4}$', date_str):
                return datetime.strptime(date_str, '%d/%m/%Y')

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur parsing date '{date_str}': {e}")

        return None

    def update_document_metadata(self, document: Document, metadata: Dict) -> Dict[str, any]:
        """Met √† jour un document avec les m√©tadonn√©es extraites - VERSION CORRIG√âE"""
        update_result = {
            'fields_updated': [],
            'ema_data_found': False,
            'errors': []
        }

        try:
            # CORRECTION CRITIQUE : Traitement prioritaire du titre
            extracted_title = metadata.get('title', '').strip()
            logger.info(f"üîç Titre √† sauvegarder: '{extracted_title}'")

            if extracted_title and extracted_title.lower() not in ['document sans titre', 'untitled', '', 'none']:
                # Sauvegarder le titre extrait par l'IA
                old_title = getattr(document, 'extracted_title', '') or ''
                new_title = self.safe_string_field(extracted_title, 255)

                if new_title != old_title:
                    document.extracted_title = new_title
                    update_result['fields_updated'].append('extracted_title')
                    logger.info(f"‚úÖ Titre sauvegard√© dans document.extracted_title: '{new_title}'")
                else:
                    logger.info(f"üìù Titre d√©j√† correct: '{new_title}'")
            else:
                logger.warning(f"‚ö†Ô∏è Titre invalide non sauvegard√©: '{extracted_title}'")

            # Mise √† jour des autres champs de base
            field_mappings = {
                'language': ('language', 10),
                'source': ('source', 255),
                'version': ('version', 50),
                'source_url': ('source_url', 500)
            }

            for doc_field, (meta_key, max_length) in field_mappings.items():
                if meta_key in metadata and metadata[meta_key]:
                    old_value = getattr(document, doc_field, '') or ''
                    new_value = self.safe_string_field(metadata[meta_key], max_length)

                    if new_value and new_value != old_value:
                        setattr(document, doc_field, new_value)
                        update_result['fields_updated'].append(doc_field)
                        logger.debug(f"üìù {doc_field} mis √† jour: '{old_value}' -> '{new_value}'")

            # Date de publication
            if metadata.get('publication_date'):
                parsed_date = self.parse_date(metadata['publication_date'])
                if parsed_date:
                    old_date = document.publication_date
                    new_date = parsed_date.date()
                    if new_date != old_date:
                        document.publication_date = new_date
                        update_result['fields_updated'].append('publication_date')
                        logger.info(f"üìÖ Date de publication mise √† jour: {new_date}")

            # Donn√©es EMA
            ema_result = self._update_ema_data(document, metadata.get('ema_data', {}))
            update_result.update(ema_result)

            # IMPORTANT : Sauvegarder les changements
            if update_result['fields_updated'] or update_result.get('ema_fields_updated', []):
                document.save()
                total_updates = len(update_result['fields_updated']) + len(update_result.get('ema_fields_updated', []))
                logger.info(f"üíæ Document sauvegard√© avec {total_updates} champs mis √† jour")
                logger.info(f"   - Champs principaux: {update_result['fields_updated']}")
                logger.info(f"   - Champs EMA: {update_result.get('ema_fields_updated', [])}")
            else:
                logger.info("üìù Aucune modification √† sauvegarder")

        except Exception as e:
            error_msg = f"Erreur mise √† jour document: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            update_result['errors'].append(error_msg)

        return update_result

    def _update_ema_data(self, document: Document, ema_data: Dict) -> Dict[str, any]:
        """Met √† jour les donn√©es EMA sp√©cifiquement"""
        result = {
            'ema_data_found': False,
            'ema_fields_updated': []
        }

        if not isinstance(ema_data, dict):
            return result

        try:
            # Mapping des champs EMA
            ema_mappings = {
                'ema_title': ('ema_title', 255),
                'ema_source_url': ('ema_source_url', 500),
                'ema_reference': ('ema_reference', 100)
            }

            for doc_field, (ema_key, max_length) in ema_mappings.items():
                if ema_key in ema_data and ema_data[ema_key]:
                    setattr(document, doc_field, self.safe_string_field(ema_data[ema_key], max_length))
                    result['ema_fields_updated'].append(doc_field)

            # Dates EMA
            date_mappings = [
                ('original_publication_date', 'original_publication_date'),
                ('ema_publication_date', 'ema_publication_date')
            ]

            for doc_field, ema_key in date_mappings:
                if ema_key in ema_data and ema_data[ema_key]:
                    parsed_date = self.parse_date(ema_data[ema_key])
                    if parsed_date:
                        setattr(document, doc_field, parsed_date.date())
                        result['ema_fields_updated'].append(doc_field)

            # M√©tadonn√©es de recherche
            document.ema_search_performed = ema_data.get('search_performed', False)
            document.ema_similarity_score = float(ema_data.get('similarity_score', 0.0))

            if ema_data.get('search_timestamp'):
                document.ema_last_search_date = timezone.now()

            # Compter les r√©sultats
            ema_content = any([
                ema_data.get('ema_source_url'),
                ema_data.get('ema_title'),
                ema_data.get('ema_reference')
            ])

            document.ema_search_results_count = 1 if ema_content else 0
            result['ema_data_found'] = ema_content

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur mise √† jour EMA: {e}")

        return result


class ConfidenceCalculator:
    """Calculateur de scores de confiance"""

    @staticmethod
    def calculate_global_confidence(confidence_scores: Dict) -> float:
        """Calcule le score de confiance global"""
        if not confidence_scores or not isinstance(confidence_scores, dict):
            return 0.1

        weights = {
            'title': 0.25,
            'document_type': 0.15,
            'context': 0.15,
            'language': 0.15,
            'publication_date': 0.10,
            'source': 0.10,
            'version': 0.05,
            'source_url': 0.05
        }

        weighted_sum = 0
        total_weight = 0

        for field, score in confidence_scores.items():
            if field in weights:
                try:
                    score_float = float(score)
                    if 0 <= score_float <= 1:
                        weighted_sum += score_float * weights[field]
                        total_weight += weights[field]
                except (ValueError, TypeError):
                    continue

        return min(weighted_sum / total_weight, 1.0) if total_weight > 0 else 0.1


# ===== NOUVELLE FONCTION POUR TRAITER LES DOCUMENTS VIA URLs =====

@shared_task if CELERY_AVAILABLE else lambda f: f
def process_document_from_urls(document_id: int, direct_pdf_url: str, ema_page_url: str):
    """Traite un document √† partir des URLs PDF et EMA"""
    processor = URLDocumentProcessor()
    updater = DocumentUpdater()

    try:
        document = Document.objects.get(id=document_id)
        logger.info(f"üîó D√©but traitement URLs pour document {document_id}: '{document.title}'")
        logger.info(f"   PDF: {direct_pdf_url}")
        logger.info(f"   EMA: {ema_page_url}")

        # Mise √† jour du statut
        document.status = 'extracting'
        document.extraction_started_at = timezone.now()
        document.save()

        # === TRAITEMENT PRINCIPAL ===
        processing_result = processor.process_document_from_urls(direct_pdf_url, ema_page_url)

        if not processing_result['success']:
            document.status = 'refused'
            document.save()
            return {
                'success': False,
                'error': processing_result['error'],
                'document_id': document_id
            }

        # === EXTRACTION DES M√âTADONN√âES COMPL√àTES ===
        title_extraction = processing_result['title_extraction']
        ema_metadata = processing_result['ema_metadata']
        temp_file_path = processing_result['temp_file_path']

        # IMPORTANT : Ne PAS sauvegarder le titre basique de l'URL maintenant
        # Le vrai titre sera extrait par Mistral depuis le contenu du PDF
        url_based_title = title_extraction['title']  # Garder pour fallback seulement
        logger.info(f"üìù Titre basique depuis URL: '{url_based_title}' (en attente du titre Mistral)")

        # Si le fichier temporaire existe, faire l'extraction compl√®te
        full_metadata = None
        mistral_title = None

        if temp_file_path:
            try:
                text_extractor = DocumentTextExtractor()
                full_text = text_extractor.extract_text_from_file(temp_file_path, 'pdf')

                if full_text and len(full_text.strip()) > 50:
                    # Extraction compl√®te des m√©tadonn√©es avec Mistral
                    metadata_service = MistralAIService()
                    full_metadata = metadata_service.extract_metadata_with_confidence(
                        text=full_text,
                        file_type='pdf',
                        document_title=url_based_title,  # Passer le titre URL comme hint
                        source_url=direct_pdf_url
                    )

                    # CORRECTION CRITIQUE : R√©cup√©rer le titre extrait par Mistral
                    mistral_title = full_metadata.get('title', '').strip()
                    logger.info(f"ü§ñ Titre extrait par Mistral: '{mistral_title}'")

                    # Remplacer les donn√©es EMA par celles extraites de la page
                    if ema_metadata['success']:
                        full_metadata['ema_data'] = ema_metadata

                    logger.info(f"üìä M√©tadonn√©es compl√®tes extraites")
                else:
                    logger.warning("‚ö†Ô∏è Texte insuffisant pour extraction compl√®te")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur extraction m√©tadonn√©es compl√®tes: {e}")
            finally:
                # Nettoyer le fichier temporaire
                processor.cleanup_processing_files(temp_file_path)

        # === MISE √Ä JOUR DU DOCUMENT ===
        update_result = {'fields_updated': [], 'ema_data_found': False}

        # D√©terminer le titre final √† utiliser
        final_title = ''
        if mistral_title and mistral_title.lower() not in ['document sans titre', 'untitled', '', 'none']:
            final_title = mistral_title
            logger.info(f"‚úÖ Utilisation du titre Mistral: '{final_title}'")
        else:
            final_title = url_based_title
            logger.warning(f"‚ö†Ô∏è Fallback vers titre URL: '{final_title}'")

        if full_metadata:
            # S'assurer que le titre correct est dans les m√©tadonn√©es
            full_metadata['title'] = final_title
            logger.info(f"üìù Titre dans full_metadata avant update: '{full_metadata.get('title', '')}'")
            update_result = updater.update_document_metadata(document, full_metadata)
        else:
            # Mise √† jour minimale avec les donn√©es extraites
            document.extracted_title = final_title
            update_result['fields_updated'].append('extracted_title')

            # Donn√©es EMA si disponibles
            if ema_metadata['success']:
                ema_result = updater._update_ema_data(document, ema_metadata)
                update_result.update(ema_result)

            # Sauvegarder manuellement si pas de full_metadata
            document.save()

        # === FINALISATION ===
        document.status = 'extracted'
        document.extraction_completed_at = timezone.now()
        document.save()

        # === CR√âATION R√âSULTAT EXTRACTION ===
        processing_time = (timezone.now() - document.extraction_started_at).total_seconds()

        # Scores de confiance
        confidence_scores = {}
        global_confidence = 0.6  # Score par d√©faut pour traitement URLs

        if full_metadata and 'confidence_scores' in full_metadata:
            confidence_scores = full_metadata['confidence_scores']
            global_confidence = ConfidenceCalculator.calculate_global_confidence(confidence_scores)

        # M√©tadonn√©es enrichies - UTILISER LE BON TITRE
        if full_metadata:
            enriched_metadata = full_metadata.copy()
            enriched_metadata['title'] = final_title  # S'assurer que le bon titre est utilis√©
            logger.info(f"‚úÖ M√©tadonn√©es enrichies avec titre Mistral: '{enriched_metadata['title']}'")
        else:
            enriched_metadata = {
                'title': final_title,
                'source': 'EMA',
                'language': 'en',
                'extraction_method': 'url_processing'
            }
            logger.info(f"üìù M√©tadonn√©es minimales avec titre: '{enriched_metadata['title']}'")

        enriched_metadata['extraction_stats'] = {
            'processing_time_seconds': processing_time,
            'fields_updated': update_result['fields_updated'],
            'ema_data_found': update_result['ema_data_found'],
            'extraction_method': 'url_processing_v1',
            'title_extraction_confidence': title_extraction.get('confidence', 0.0),
            'ema_similarity_score': ema_metadata.get('similarity_score', 0.0),
            'direct_pdf_url': direct_pdf_url,
            'ema_page_url': ema_page_url,
            'mistral_title_extracted': bool(mistral_title)
        }

        try:
            extraction_result_obj = ExtractionResult.objects.create(
                document=document,
                extracted_data=enriched_metadata,
                confidence_scores=confidence_scores,
                confidence_score=global_confidence,
                status='completed',
                extraction_method='url_processing_v1',
                model_version='v1.0'
            )
            logger.info(f"üìä ExtractionResult cr√©√© (ID: {extraction_result_obj.id})")

            # V√©rification finale
            saved_title = extraction_result_obj.extracted_data.get('title', 'NON TROUV√â')
            logger.info(f"üîç Titre sauvegard√© dans ExtractionResult: '{saved_title}'")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur cr√©ation ExtractionResult: {e}")

        # === ANNOTATION AUTOMATIQUE ===
        try:
            if CELERY_AVAILABLE:
                auto_annotate_document.delay(document_id)
            else:
                auto_annotate_document(document_id)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur annotation automatique: {e}")

        # === R√âSUM√â ===
        logger.info(f"üéâ Traitement URLs termin√© avec succ√®s pour document {document_id}")
        logger.info(f"   - Titre final: '{final_title}'")
        logger.info(f"   - Titre Mistral extrait: {'‚úÖ' if mistral_title else '‚ùå'}")
        logger.info(f"   - Donn√©es EMA: {'‚úÖ' if update_result['ema_data_found'] else '‚ùå'}")
        logger.info(f"   - Score confiance: {global_confidence:.3f}")
        logger.info(f"   - Temps de traitement: {processing_time:.2f}s")

        return {
            'success': True,
            'document_id': document_id,
            'title_extracted': final_title,  # Retourner le titre final (Mistral ou fallback)
            'ema_data_found': update_result['ema_data_found'],
            'confidence_score': global_confidence,
            'processing_time': processing_time,
            'fields_updated': update_result['fields_updated']
        }

    except Document.DoesNotExist:
        logger.error(f"‚ùå Document {document_id} non trouv√©")
        return {'success': False, 'error': f'Document {document_id} non trouv√©'}

    except Exception as e:
        logger.error(f"‚ùå Erreur critique traitement URLs document {document_id}: {e}")

        try:
            document = Document.objects.get(id=document_id)
            document.status = 'refused'
            document.extraction_completed_at = timezone.now()
            document.save()
        except:
            pass

        return {'success': False, 'error': str(e)}


def process_metadata_extraction(self, text: str, file_type: str, document_title: str, document=None) -> Dict:
    """Traite l'extraction des m√©tadonn√©es avec fallback - AVEC URL EMA"""
    try:
        # R√©cup√©rer l'URL EMA si disponible
        ema_page_url = None
        if document and hasattr(document, 'ema_page_url'):
            ema_page_url = document.ema_page_url
            logger.info(f"üîó URL EMA disponible: {ema_page_url}")

        metadata = self.metadata_service.extract_metadata_with_ema(
            text=text,
            file_type=file_type,
            document_title=document_title,
            ema_page_url=ema_page_url
        )

        # V√©rification du titre
        extracted_title = metadata.get('title', '').strip()
        logger.info(f"üîç Titre dans metadata apr√®s extraction: '{extracted_title}'")

        if extracted_title and extracted_title.lower() not in ['document sans titre', 'untitled', '', 'none']:
            logger.info(f"‚úÖ Titre valide pr√©serv√©: '{extracted_title}'")
        else:
            logger.warning(f"‚ö†Ô∏è Titre invalide apr√®s extraction: '{extracted_title}'")
            if document_title and document_title.lower() not in ['document sans titre', 'untitled', '']:
                metadata['title'] = document_title
                logger.info(f"üîÑ Fallback titre document: '{document_title}'")

        return metadata

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è √âchec extraction m√©tadonn√©es, utilisation fallback: {e}")
        return self._get_fallback_metadata(document_title, str(e))


@shared_task if CELERY_AVAILABLE else lambda f: f
def extract_document_metadata(document_id: int):
    """Extraction de m√©tadonn√©es - FLUX DE DONN√âES CORRIG√â AVEC EMA"""
    processor = DocumentProcessor()
    updater = DocumentUpdater()

    try:
        document = Document.objects.get(id=document_id)
        logger.info(f"üöÄ D√©but extraction classique pour document {document_id}: '{document.title}'")

        # V√©rifier si on a une URL EMA
        if hasattr(document, 'ema_page_url') and document.ema_page_url:
            logger.info(f"üîó URL EMA disponible: {document.ema_page_url}")

        # === VALIDATION ===
        validation = processor.validate_document(document)
        if not validation['valid']:
            document.status = 'refused'
            document.save()
            return {'success': False, 'error': validation['error']}

        # Si le document a des URLs mais pas de fichier, rediriger vers traitement URLs
        if validation['has_urls'] and not validation['file_path']:
            logger.info(f"üîó Redirection vers traitement URLs pour document {document_id}")
            return process_document_from_urls(document_id, document.direct_pdf_url, document.ema_page_url)

        # Mise √† jour du statut
        document.status = 'extracting'
        document.extraction_started_at = timezone.now()
        document.save()

        logger.info(f"üìÅ Fichier valid√©: {validation['file_size']} bytes")

        # === EXTRACTION TEXTE ===
        text_result = processor.extract_text_with_validation(
            validation['file_path'],
            document.file_type or 'unknown'
        )

        if not text_result['success']:
            document.status = 'refused'
            document.save()
            return {'success': False, 'error': text_result['error']}

        logger.info(f"‚úÖ Texte extrait: {text_result['length']} caract√®res")

        # === EXTRACTION M√âTADONN√âES AVEC DOCUMENT ===
        metadata = processor.process_metadata_extraction(
            text_result['text'],
            document.file_type or 'unknown',
            document.title,
            document=document  # IMPORTANT: Passer l'objet document pour acc√©der √† ema_page_url
        )

        logger.info(f"üìä M√©tadonn√©es extraites: {metadata.get('title', 'N/A')[:50]}...")

        # Log des donn√©es EMA extraites
        if 'ema_data' in metadata:
            logger.info(f"üìä Donn√©es EMA extraites: {metadata['ema_data']}")

        # CORRECTION CRITIQUE : V√©rifier le titre extrait par le LLM
        llm_title = metadata.get('title', '').strip()
        logger.info(f"üîç Titre LLM dans metadata: '{llm_title}'")

        # === MISE √Ä JOUR DOCUMENT ===
        update_result = updater.update_document_metadata(document, metadata)

        # === FINALISATION ===
        document.status = 'extracted'
        document.extraction_completed_at = timezone.now()
        document.save()

        # === CR√âATION R√âSULTAT EXTRACTION - STRUCTURE PLATE CORRIG√âE ===
        confidence_scores = metadata.get('confidence_scores', {})
        global_confidence = ConfidenceCalculator.calculate_global_confidence(confidence_scores)

        processing_time = (timezone.now() - document.extraction_started_at).total_seconds()

        # CORRECTION CRITIQUE : Inclure TOUTES les donn√©es EMA dans la structure plate
        extracted_data_flat = {
            'title': llm_title,  # UTILISER DIRECTEMENT le titre du LLM
            'document_type': metadata.get('document_type', ''),
            'context': metadata.get('context', ''),
            'language': metadata.get('language', 'fr'),
            'publication_date': metadata.get('publication_date', ''),
            'source': metadata.get('source', ''),
            'version': metadata.get('version', ''),
            'source_url': metadata.get('source_url', ''),
            'country': metadata.get('country', ''),
            # IMPORTANT : Inclure TOUTES les donn√©es EMA
            'ema_data': metadata.get('ema_data', {}),
            # ET AUSSI les champs EMA individuellement pour compatibilit√© avec l'interface
            'original_publication_date': metadata.get('ema_data', {}).get('original_publication_date'),
            'ema_publication_date': metadata.get('ema_data', {}).get('ema_publication_date'),
            'ema_source_url': metadata.get('ema_data', {}).get('ema_source_url', ''),
            'ema_title': metadata.get('ema_data', {}).get('ema_title', ''),
            'ema_reference': metadata.get('ema_data', {}).get('ema_reference', ''),
            # Stats d'extraction
            'extraction_stats': {
                'text_length': text_result['length'],
                'processing_time_seconds': processing_time,
                'fields_updated': update_result['fields_updated'],
                'ema_data_found': update_result['ema_data_found'],
                'extraction_method': 'file_processing_v1',
                'extraction_reasoning': metadata.get('extraction_reasoning', {}),
                'ema_page_url_used': document.ema_page_url if hasattr(document, 'ema_page_url') else None
            }
        }

        # LOGS DE V√âRIFICATION EMA
        logger.info(f"üìä Donn√©es EMA dans extracted_data_flat:")
        logger.info(f"   - ema_data complet: {extracted_data_flat.get('ema_data', {})}")
        logger.info(f"   - ema_title: '{extracted_data_flat.get('ema_title', '')}'")
        logger.info(f"   - ema_source_url: '{extracted_data_flat.get('ema_source_url', '')}'")
        logger.info(f"   - ema_reference: '{extracted_data_flat.get('ema_reference', '')}'")
        logger.info(f"   - original_publication_date: {extracted_data_flat.get('original_publication_date')}")
        logger.info(f"   - ema_publication_date: {extracted_data_flat.get('ema_publication_date')}")

        logger.info(f"üíæ Titre dans extracted_data_flat: '{extracted_data_flat['title']}'")

        # V√âRIFICATION FINALE avant sauvegarde
        if not extracted_data_flat['title'] or extracted_data_flat['title'].lower() in ['document sans titre',
                                                                                        'untitled', '']:
            logger.warning(f"‚ö†Ô∏è Titre invalide d√©tect√© avant sauvegarde: '{extracted_data_flat['title']}'")
            # Essayer d'utiliser le titre du document comme fallback
            fallback_title = document.extracted_title or document.title
            if fallback_title and fallback_title.lower() not in ['document sans titre', 'untitled']:
                extracted_data_flat['title'] = fallback_title
                logger.info(f"üîÑ Fallback titre appliqu√©: '{fallback_title}'")

        logger.info(f"‚úÖ Titre final pour sauvegarde: '{extracted_data_flat['title']}'")

        try:
            extraction_result_obj = ExtractionResult.objects.create(
                document=document,
                extracted_data=extracted_data_flat,  # Structure plate avec le bon titre ET les donn√©es EMA
                confidence_scores=confidence_scores,
                confidence_score=global_confidence,
                status='completed',
                extraction_method='file_processing_v1',
                model_version='v1.0'
            )
            logger.info(f"üìä ExtractionResult cr√©√© (ID: {extraction_result_obj.id})")
            logger.info(f"‚úÖ Titre sauvegard√© dans ExtractionResult: '{extracted_data_flat['title']}'")

            # V√âRIFICATION POST-SAUVEGARDE incluant EMA
            saved_data = extraction_result_obj.extracted_data
            saved_title = saved_data.get('title', 'NON TROUV√â') if isinstance(saved_data, dict) else 'ERREUR TYPE'
            saved_ema_title = saved_data.get('ema_title', 'NON TROUV√â') if isinstance(saved_data,
                                                                                      dict) else 'ERREUR TYPE'
            logger.info(f"üîç V√©rification post-sauvegarde:")
            logger.info(f"   - Titre: '{saved_title}'")
            logger.info(f"   - Titre EMA: '{saved_ema_title}'")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur cr√©ation ExtractionResult: {e}")

        # === ANNOTATION AUTOMATIQUE ===
        try:
            if CELERY_AVAILABLE:
                auto_annotate_document.delay(document_id)
            else:
                auto_annotate_document(document_id)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur annotation automatique: {e}")

        # === R√âSUM√â ===
        logger.info(f"üéâ Extraction classique termin√©e avec succ√®s pour document {document_id}")
        logger.info(f"   - Score confiance: {global_confidence:.3f}")
        logger.info(f"   - Donn√©es EMA: {'‚úÖ' if update_result['ema_data_found'] else '‚ùå'}")
        logger.info(f"   - Temps de traitement: {processing_time:.2f}s")

        return {
            'success': True,
            'metadata': extracted_data_flat,
            'confidence_score': global_confidence,
            'ema_data_found': update_result['ema_data_found'],
            'document_id': document_id,
            'processing_time': processing_time
        }

    except Document.DoesNotExist:
        logger.error(f"‚ùå Document {document_id} non trouv√©")
        return {'success': False, 'error': f'Document {document_id} non trouv√©'}

    except Exception as e:
        logger.error(f"‚ùå Erreur critique extraction document {document_id}: {e}")

        try:
            document = Document.objects.get(id=document_id)
            document.status = 'refused'
            document.extraction_completed_at = timezone.now()
            document.save()
        except:
            pass

        return {'success': False, 'error': str(e)}


@shared_task if CELERY_AVAILABLE else lambda f: f
def re_extract_document_metadata(document_id: int):
    """R√©-extraction avec reset complet - FORCE RESET EMA"""
    try:
        document = Document.objects.get(id=document_id)
        logger.info(f"üîÑ R√©-extraction FORC√âE pour document {document_id}")

        # === SUPPRESSION COMPL√àTE DES ANCIENNES DONN√âES ===
        old_results_count = document.extraction_results.count()
        document.extraction_results.all().delete()
        logger.info(f"üóëÔ∏è {old_results_count} anciens r√©sultats d'extraction supprim√©s")

        # === RESET COMPLET ET FORC√â ===
        _reset_document_data(document)

        # === V√âRIFICATION POST-RESET ===
        document.refresh_from_db()
        logger.info(f"üîç V√©rification post-reset:")
        logger.info(f"  - ema_search_performed: {document.ema_search_performed}")
        logger.info(f"  - ema_title: '{document.ema_title}'")
        logger.info(f"  - ema_source_url: '{document.ema_source_url}'")
        logger.info(f"  - Extraction results count: {document.extraction_results.count()}")

        # === RELANCER EXTRACTION AVEC DONN√âES PROPRES ===
        logger.info(f"üöÄ Lancement nouvelle extraction avec donn√©es propres")
        result = extract_document_metadata(document_id)

        if result.get('success'):
            logger.info(f"‚úÖ R√©-extraction r√©ussie pour document {document_id}")

            # V√©rifier les NOUVELLES donn√©es EMA
            new_metadata = result.get('metadata', {})
            new_ema_data = new_metadata.get('ema_data', {})
            new_title = new_metadata.get('title', 'N/A')

            logger.info(f"üìä NOUVELLES donn√©es apr√®s r√©-extraction:")
            logger.info(f"  - Nouveau titre: '{new_title}'")
            logger.info(f"  - Nouvelles donn√©es EMA: {new_ema_data}")

            # V√©rification finale des donn√©es sauvegard√©es
            document.refresh_from_db()
            logger.info(f"üìä Donn√©es EMA finales dans la DB:")
            logger.info(f"  - ema_title: '{document.ema_title}'")
            logger.info(f"  - ema_source_url: '{document.ema_source_url}'")
            logger.info(f"  - original_publication_date: {document.original_publication_date}")

        else:
            logger.error(f"‚ùå R√©-extraction √©chou√©e: {result.get('error')}")

        return result

    except Document.DoesNotExist:
        logger.error(f"‚ùå Document {document_id} non trouv√© pour r√©-extraction")
        return {'success': False, 'error': f'Document {document_id} non trouv√©'}
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©-extraction: {e}")
        return {'success': False, 'error': str(e)}


def _reset_document_data(document: Document):
    """Reset toutes les donn√©es d'un document - VERSION D√âFINITIVE"""
    logger.info(f"üîÑ Reset COMPLET des donn√©es pour document {document.id}")

    # Reset EXPLICITE de tous les champs EMA
    document.ema_title = ''
    document.ema_source_url = ''
    document.ema_reference = ''
    document.original_publication_date = None
    document.ema_publication_date = None
    document.ema_search_performed = False
    document.ema_similarity_score = 0.0
    document.ema_search_results_count = 0
    document.ema_last_search_date = None

    # Reset autres champs m√©tadonn√©es
    document.extracted_title = ''
    document.language = ''
    document.source = ''
    document.version = ''
    document.source_url = ''
    document.publication_date = None

    # Reset statut
    document.status = 'pending'
    document.extraction_started_at = None
    document.extraction_completed_at = None

    document.save()

    # V√©rification imm√©diate
    document.refresh_from_db()
    logger.info(f"‚úÖ Reset termin√© pour document {document.id}")
    logger.info(f"  V√©rifications post-reset:")
    logger.info(f"    - ema_title: '{document.ema_title}'")
    logger.info(f"    - ema_source_url: '{document.ema_source_url}'")
    logger.info(f"    - ema_search_performed: {document.ema_search_performed}")


@shared_task if CELERY_AVAILABLE else lambda f: f
def auto_annotate_document(document_id: int):
    """Annotation automatique optimis√©e"""
    processor = DocumentProcessor()

    try:
        document = Document.objects.get(id=document_id)
        logger.info(f"üìù Annotation automatique pour document {document_id}")

        validation = processor.validate_document(document)
        if not validation['valid']:
            return {'success': False, 'error': validation['error']}

        # Extraction de texte selon le mode
        if validation['file_path']:
            # Mode fichier local
            text_result = processor.extract_text_with_validation(
                validation['file_path'],
                document.file_type or 'unknown',
                min_length=50
            )
        elif validation['has_urls']:
            # Mode URLs - t√©l√©charger temporairement
            from .url_services import URLDocumentExtractor
            url_extractor = URLDocumentExtractor()
            temp_file = url_extractor.download_pdf_from_url(document.direct_pdf_url)

            if temp_file:
                text_result = processor.extract_text_with_validation(temp_file, 'pdf', min_length=50)
                url_extractor.cleanup_temp_file(temp_file)
            else:
                return {'success': False, 'error': 'Impossible de t√©l√©charger le PDF pour annotation'}
        else:
            return {'success': False, 'error': 'Aucune source de texte disponible'}

        if not text_result['success']:
            return {'success': False, 'error': text_result['error']}

        annotations = processor.nlp_service.auto_annotate_document(text_result['text'])
        saved_count = _save_annotations(document, annotations)

        logger.info(f"‚úÖ {saved_count}/{len(annotations)} annotations sauvegard√©es")

        return {
            'success': True,
            'annotations_count': saved_count,
            'annotations_detected': len(annotations)
        }

    except Document.DoesNotExist:
        return {'success': False, 'error': f'Document {document_id} non trouv√©'}
    except Exception as e:
        logger.error(f"‚ùå Erreur annotation: {e}")
        return {'success': False, 'error': str(e)}


def _save_annotations(document: Document, annotations: List[Dict]) -> int:
    """Sauvegarde les annotations avec gestion d'erreurs"""
    saved_count = 0

    try:
        from annotation.models import Annotation, EntityType
        from accounts.models import User

        # Utilisateur syst√®me
        system_user = User.objects.filter(username='system').first()
        if not system_user:
            system_user = User.objects.filter(is_superuser=True).first()

        if not system_user:
            logger.warning("‚ö†Ô∏è Aucun utilisateur syst√®me pour annotations")
            return 0

        # Supprimer anciennes annotations automatiques
        Annotation.objects.filter(document=document, is_automatic=True).delete()

        for ann_data in annotations:
            try:
                if not ann_data.get('text') or not ann_data.get('entity_type'):
                    continue

                entity_type, _ = EntityType.objects.get_or_create(
                    name=ann_data['entity_type'],
                    defaults={
                        'color': '#007bff',
                        'description': f'Entit√© {ann_data["entity_type"]} d√©tect√©e automatiquement'
                    }
                )

                Annotation.objects.create(
                    document=document,
                    entity_type=entity_type,
                    text=ann_data['text'][:255],
                    start_position=ann_data.get('start_position', 0),
                    end_position=ann_data.get('end_position', 0),
                    confidence_score=min(ann_data.get('confidence_score', 0.7), 1.0),
                    created_by=system_user,
                    is_automatic=True
                )
                saved_count += 1

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur annotation '{ann_data.get('text', '')}': {e}")
                continue

    except ImportError:
        logger.warning("‚ö†Ô∏è Module annotation non disponible")
    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©rale annotations: {e}")

    return saved_count